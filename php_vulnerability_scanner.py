import os
import socket
import subprocess
import requests
from bs4 import BeautifulSoup
import colorama
from colorama import Fore, Style
from urllib.parse import urljoin, urlparse, parse_qs
import urllib3
import chardet
import random
import cloudscraper
import ssl
from requests.adapters import HTTPAdapter
from urllib3.poolmanager import PoolManager

# Disable SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

colorama.init(autoreset=True)

visited_links = set()
output_file = 'link_extract.txt'
nmap_path = None
user_agents = []
use_random_user_agent = False
cloudflare_bypass = False  # Flag to indicate Cloudflare bypass
scraper = requests  # Default to normal requests library

# Custom HTTPS Adapter to disable hostname check for 'verify=False'
class SSLAdapter(HTTPAdapter):
    def init_poolmanager(self, *args, **kwargs):
        context = ssl.create_default_context()
        context.check_hostname = False  # Disable hostname verification
        context.verify_mode = ssl.CERT_NONE  # Disable certificate verification
        kwargs['ssl_context'] = context
        return super(SSLAdapter, self).init_poolmanager(*args, **kwargs)

# Function to load user agents from file
def load_user_agents(file_path):
    global user_agents
    if os.path.exists(file_path):
        with open(file_path, 'r') as f:
            user_agents = [line.strip() for line in f if line.strip()]
    else:
        print(f"{Fore.RED}[!] User agent file not found: {file_path}")

# Function to get a random user agent
def get_random_user_agent():
    if user_agents:
        return random.choice(user_agents)
    return None

# Function to decide if random user agents will be used
def ask_for_random_user_agent():
    global use_random_user_agent
    user_input = input(f"{Fore.CYAN}Do you want to use random user agents for requests? (y/n): ").strip().lower()
    if user_input == 'y':
        use_random_user_agent = True
        load_user_agents('user_agent.txt')
    else:
        print(f"{Fore.YELLOW}[*] Random user agent usage disabled.")

# Function to set headers with a random or default user agent
def get_headers():
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
    }
    if use_random_user_agent:
        random_agent = get_random_user_agent()
        if random_agent:
            headers["User-Agent"] = random_agent
    return headers

# Function to get the domain name
def get_domain():
    domain = input(f"{Fore.CYAN}Enter the domain name (e.g., example.com): ").strip()
    return f"http://{domain}" if not domain.startswith("http") else domain

# Function to search for Nmap in common locations
def find_nmap_in_c_drive():
    print(f"{Fore.CYAN}[*] Searching for Nmap in C:\\ drive...")

    common_dirs = ['C:\\', 'C:\\Program Files', 'C:\\Program Files (x86)']
    for directory in common_dirs:
        for root, dirs, files in os.walk(directory):
            for dir_name in dirs:
                if "nmap" in dir_name.lower():
                    potential_path = os.path.join(root, dir_name)
                    nmap_exe_path = os.path.join(potential_path, "nmap.exe")
                    if os.path.exists(nmap_exe_path):
                        print(f"{Fore.GREEN}[+] Nmap found at: {nmap_exe_path}")
                        return nmap_exe_path
    return None

# Function to handle Nmap detection and user input
def handle_nmap_detection():
    global nmap_path

    user_input = input(f"{Fore.CYAN}Do you want to scan ports using Nmap? (y/n): ").strip().lower()
    if user_input != 'y':
        print(f"{Fore.YELLOW}[*] Skipping Nmap port scan.")
        return None

    nmap_path = find_nmap_in_c_drive()

    if not nmap_path:
        print(f"{Fore.RED}[!] Nmap not found in default directories.")
        user_input = input(f"{Fore.CYAN}Do you want to provide the Nmap installation directory? (y/n): ").strip().lower()
        if user_input == 'y':
            user_path = input(f"{Fore.CYAN}Please provide the path to Nmap (e.g., C:\\Nmap\\nmap.exe): ").strip()
            if os.path.exists(user_path) and user_path.endswith("nmap.exe"):
                print(f"{Fore.GREEN}[+] Nmap found at: {user_path}")
                nmap_path = user_path
            else:
                print(f"{Fore.RED}[!] Invalid path. Skipping Nmap port scan.")
        else:
            print(f"{Fore.YELLOW}[*] Nmap usage skipped. Continuing without Nmap.")
            return None

    return nmap_path

# Function to detect IP address and optionally scan ports using Nmap
def detect_ip_and_ports(domain):
    global nmap_path

    try:
        ip_address = socket.gethostbyname(domain)
        print(f"{Fore.YELLOW}[+] IP Address of {domain}: {ip_address}")

        if nmap_path:
            print(f"{Fore.CYAN}[*] Scanning open ports on {ip_address} using Nmap...")
            subprocess.run([nmap_path, '-p-', ip_address], check=True, timeout=300)
        else:
            print(f"{Fore.YELLOW}[!] Nmap is not available. Skipping port scanning.")

        return ip_address
    except subprocess.TimeoutExpired:
        print(f"{Fore.RED}[!] Nmap scan took too long and timed out. Skipping port scanning.")
    except PermissionError as e:
        print(f"{Fore.RED}[!] Error detecting IP and ports: {e}")
        print(f"{Fore.YELLOW}[!] Try running the program as administrator to get the necessary permissions.")
        return None
    except Exception as e:
        print(f"{Fore.RED}[!] Error detecting IP and ports: {e}")
        return None

# Function to detect if the site is PHP-based
def is_php_site(base_url):
    try:
        response = send_request(base_url)
        headers = response.headers
        if 'X-Powered-By' in headers and 'PHP' in headers['X-Powered-By']:
            print(f"{Fore.YELLOW}[+] The website is using PHP")
            return True
        if 'php' in response.text.lower():
            print(f"{Fore.YELLOW}[+] The website appears to be PHP-based (found 'php' in the content)")
            return True
        else:
            print(f"{Fore.YELLOW}[!] The website does not appear to be using PHP")
            return False
    except requests.exceptions.RequestException as e:
        print(f"{Fore.RED}[!] Error checking if the site is PHP: {e}")
        return False

# Function to auto-detect and fix encoding issues
def fix_encoding(content):
    result = chardet.detect(content)
    encoding = result['encoding']
    if encoding:
        return content.decode(encoding, errors='ignore')
    return content.decode('utf-8', errors='ignore')

# Function to detect CMS (WordPress, Joomla, Drupal, etc.)
def detect_cms(base_url):
    try:
        response = send_request(base_url)
        decoded_content = fix_encoding(response.content)

        if 'wp-content' in decoded_content or 'wordpress' in decoded_content.lower():
            print(f"{Fore.YELLOW}[+] WordPress CMS detected")
            return "WordPress"
        elif 'joomla' in decoded_content.lower():
            print(f"{Fore.YELLOW}[+] Joomla CMS detected")
            return "Joomla"
        elif 'drupal' in decoded_content.lower():
            print(f"{Fore.YELLOW}[+] Drupal CMS detected")
            return "Drupal"
        else:
            print(f"{Fore.YELLOW}[!] No known CMS detected")
            return None
    except requests.exceptions.RequestException as e:
        print(f"{Fore.RED}[!] Error detecting CMS: {e}")
        return None

# Function to detect the server type (Apache, Nginx, etc.)
def detect_server(base_url):
    try:
        response = send_request(base_url)
        server = response.headers.get('Server')
        if server:
            print(f"{Fore.YELLOW}[+] Server detected: {server}")
        else:
            print(f"{Fore.YELLOW}[!] Could not detect server")
        return server
    except requests.exceptions.RequestException as e:
        print(f"{Fore.RED}[!] Error detecting server: {e}")
        return None

# Function to crawl and find PHP pages with query parameters
def crawl_and_find_php_pages(base_url, current_url, depth=0, max_depth=10):
    if current_url in visited_links or depth > max_depth:
        return []

    visited_links.add(current_url)

    try:
        response = send_request(current_url)
        decoded_content = fix_encoding(response.content)
        soup = BeautifulSoup(decoded_content, 'lxml')  # Switch to a more lenient parser (lxml)
    except requests.exceptions.RequestException as e:
        print(f"{Fore.RED}[!] Error fetching {current_url}: {e}")
        return []
    except Exception as e:
        print(f"{Fore.RED}[!] Error parsing {current_url}: {e}")
        return []

    php_links = []

    for link in soup.find_all('a', href=True):
        href = link['href']
        full_link = urljoin(base_url, href)

        if base_url in full_link and full_link not in visited_links:
            if '.php' in href:
                parsed_url = urlparse(full_link)
                query_params = parse_qs(parsed_url.query)

                if query_params:
                    print(f"{Fore.GREEN}[+] Found PHP page with parameters: {full_link}")
                    php_links.append(full_link)

                    with open(output_file, 'a') as file:
                        file.write(full_link + '\n')

            php_links.extend(crawl_and_find_php_pages(base_url, full_link, depth + 1))

    return php_links

# Function to test for SQL vulnerabilities using SQLMap
def check_sql_vulnerability(links):
    print(f"{Fore.CYAN}\n[*] Checking for SQL vulnerabilities using SQLMap...")

    vulnerable_links = []

    for link in links:
        if '?' in link:
            try:
                print(f"{Fore.YELLOW}[*] Running SQLMap on: {link}")
                subprocess.run(['sqlmap', '-u', link, '--batch'], check=True)
                vulnerable_links.append(link)
            except subprocess.CalledProcessError:
                print(f"{Fore.RED}[!] No SQL injection vulnerability detected for {link}")
                continue

    return vulnerable_links

# Function to send HTTP requests with or without Cloudflare bypass
def send_request(url):
    global scraper

    return scraper.get(url, headers=get_headers(), verify=False, timeout=10)

# Function to handle Cloudflare detection and bypass
def handle_cloudflare_detection(base_url):
    global scraper

    try:
        response = requests.get(base_url, headers=get_headers(), verify=False, timeout=10)
        if "cloudflare" in response.text.lower() or response.status_code in [403, 503]:
            print(f"{Fore.RED}[!] Cloudflare detected. Scan may be interrupted.")
            user_input = input(f"{Fore.CYAN}Do you want to bypass Cloudflare? (y/n): ").strip().lower()
            if user_input == 'y':
                print(f"{Fore.YELLOW}[*] Bypassing Cloudflare protection...")
                scraper = cloudscraper.create_scraper()
                scraper.mount('https://', SSLAdapter())  # Disable hostname verification
                scraper.mount('http://', SSLAdapter())  # Disable hostname verification
                return True
            else:
                print(f"{Fore.YELLOW}[*] Skipping Cloudflare bypass.")
                return False
        return False
    except requests.exceptions.RequestException as e:
        print(f"{Fore.RED}[!] Error during Cloudflare detection: {e}")
        return False

# Main function
def main():
    base_url = get_domain()

    # Step 1: Ask if random user agents should be used
    ask_for_random_user_agent()

    # Step 2: Detect Cloudflare protection and ask user to bypass if detected
    handle_cloudflare_detection(base_url)

    # Step 3: Detect Nmap installation and ask user input if needed
    handle_nmap_detection()

    # Step 4: Detect IP address and optionally scan ports
    ip_address = detect_ip_and_ports(urlparse(base_url).hostname)

    # Step 5: Check if the website is PHP-based
    if not is_php_site(base_url):
        return

    # Step 6: Detect the CMS if available
    detect_cms(base_url)

    # Step 7: Detect the server and OS version
    detect_server(base_url)

    # Step 8: Crawl and find PHP-related pages with parameters
    global visited_links
    visited_links = set()

    with open(output_file, 'w') as file:
        file.write('')

    print(f"{Fore.CYAN}[*] Crawling {base_url}")
    php_links = crawl_and_find_php_pages(base_url, base_url)
    if not php_links:
        print(f"{Fore.RED}[!] No PHP pages with parameters found.")
        return

    # Step 9: Check for SQL Injection vulnerabilities
    check_sql_vulnerability(php_links)

if __name__ == '__main__':
    main()
